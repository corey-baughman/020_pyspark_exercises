{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d22cf1-5212-4c1c-85d2-70fa1812ee25",
   "metadata": {},
   "source": [
    "## Spark 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7d76d7-6594-40db-a3f0-221ba8d76245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/22 13:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2097348-40d7-4c41-80b2-c82b3b3d4e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n group\n",
       "0    0     b\n",
       "1    1     b\n",
       "2    2     c\n",
       "3    3     a\n",
       "4    4     c\n",
       "5    5     c\n",
       "6    6     a\n",
       "7    7     b\n",
       "8    8     a\n",
       "9    9     b\n",
       "10  10     b\n",
       "11  11     a\n",
       "12  12     b\n",
       "13  13     a\n",
       "14  14     b\n",
       "15  15     b\n",
       "16  16     c\n",
       "17  17     c\n",
       "18  18     a\n",
       "19  19     c"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    dict(n=np.arange(20), group=np.random.choice(list(\"abc\"), 20))\n",
    ")\n",
    "pandas_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13425841-9814-4ecf-91d8-363192208a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[n: bigint, group: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7371b61-1a2d-4299-9b97-bf8db27faa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    b|\n",
      "|  1|    b|\n",
      "|  2|    c|\n",
      "|  3|    a|\n",
      "|  4|    c|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbf8715-db8f-4da8-81c8-05c15fac4f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----+\n",
      "|summary|                n|group|\n",
      "+-------+-----------------+-----+\n",
      "|  count|               20|   20|\n",
      "|   mean|              9.5| null|\n",
      "| stddev|5.916079783099616| null|\n",
      "|    min|                0|    a|\n",
      "|    max|               19|    c|\n",
      "+-------+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6abb8e-556e-478a-a460-53fca523097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data('mpg'))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1722ac59-eba9-46e8-a433-a8f4bd3239b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.hwy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9533934-d065-4092-bbd9-336f7ce3b562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, model: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bff4b6a-f399-4166-abb7-59b033940e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "|hwy|cty|     model|\n",
      "+---+---+----------+\n",
      "| 29| 18|        a4|\n",
      "| 29| 21|        a4|\n",
      "| 31| 20|        a4|\n",
      "| 30| 21|        a4|\n",
      "| 26| 16|        a4|\n",
      "| 26| 18|        a4|\n",
      "| 27| 18|        a4|\n",
      "| 26| 18|a4 quattro|\n",
      "| 25| 16|a4 quattro|\n",
      "| 28| 20|a4 quattro|\n",
      "+---+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6ff96a-3e32-4837-92a6-83c03ad2ef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(hwy + 1)'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.hwy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b25fbe09-5217-45d1-b005-4c7d771a6d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|hwy|(hwy + 1)|\n",
      "+---+---------+\n",
      "| 29|       30|\n",
      "| 29|       30|\n",
      "| 31|       32|\n",
      "| 30|       31|\n",
      "| 26|       27|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.hwy + 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe73eab-b836-4f25-a32f-074266a3043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|highway_mileage|\n",
      "+---------------+\n",
      "|             29|\n",
      "|             29|\n",
      "|             31|\n",
      "|             30|\n",
      "|             26|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy.alias(\"highway_mileage\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e4f90f-8b3a-4ac3-92e5-a16a313135d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = mpg.hwy.alias(\"highway_mileage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0da503c9-c94e-4f7a-9833-c1a4d34c072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|highway_mileage|highway_mileage_halved|\n",
      "+---------------+----------------------+\n",
      "|             29|                  14.5|\n",
      "|             29|                  14.5|\n",
      "|             31|                  15.5|\n",
      "|             30|                  15.0|\n",
      "|             26|                  13.0|\n",
      "+---------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col2 = (mpg.hwy / 2).alias('highway_mileage_halved')\n",
    "mpg.select(col1, col2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dca2c24-3cea-4664-b6df-025a72517036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff182ff-8e6a-4fd8-b711-c6db25931de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"hwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4c9fda-3ec4-4ad7-bf33-2cf6cbbb6726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+\n",
      "|highway_mileage|city_mileage|avg_mileage|\n",
      "+---------------+------------+-----------+\n",
      "|             29|          18|       23.5|\n",
      "|             29|          21|       25.0|\n",
      "|             31|          20|       25.5|\n",
      "|             30|          21|       25.5|\n",
      "|             26|          16|       21.0|\n",
      "+---------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_column = (col('hwy') + col('cty')) / 2\n",
    "\n",
    "mpg.select(\n",
    "    col('hwy').alias('highway_mileage'),\n",
    "    mpg.cty.alias('city_mileage'),\n",
    "    avg_column.alias('avg_mileage')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c231cc81-5c6d-42a9-979c-48e6470b02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+-------------------+\n",
      "|hwy|(hwy + 1)|highway_mileage|highway_incremented|\n",
      "+---+---------+---------------+-------------------+\n",
      "| 29|       30|             29|                 30|\n",
      "| 29|       30|             29|                 30|\n",
      "| 31|       32|             31|                 32|\n",
      "| 30|       31|             30|                 31|\n",
      "| 26|       27|             26|                 27|\n",
      "+---+---------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    expr('hwy'), # the same as 'col'\n",
    "    expr('hwy + 1'), #an arithmetic expression\n",
    "    expr('hwy AS highway_mileage'), # using an alias\n",
    "    expr('hwy +1 AS highway_incremented'), # a combination of the above\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a93c0c-5370-452f-81f5-4ec1f9f3a4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|highway|highway|highway|highway|\n",
      "+-------+-------+-------+-------+\n",
      "|     29|     29|     29|     29|\n",
      "|     29|     29|     29|     29|\n",
      "|     31|     31|     31|     31|\n",
      "|     30|     30|     30|     30|\n",
      "|     26|     26|     26|     26|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy.alias(\"highway\"),\n",
    "    col('hwy').alias('highway'),\n",
    "    expr(\"hwy\").alias('highway'),\n",
    "    expr('hwy AS highway'),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3ff4c-7f59-4f4c-a0ee-66a249b66888",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b53a4e-8a81-402a-a7c0-16752defd900",
   "metadata": {},
   "source": [
    "In order to start using spark SQL, we'll first \"register\" the table with spark:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da93ea5-445f-45e4-b25b-5742ef8b7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.createOrReplaceTempView('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c483fa60-3508-425c-a86d-dc1fdc844d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, avg: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "    FROM mpg\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bafcdea2-83bb-4ba2-b3d5-3c54f3d5c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|hwy|cty| avg|\n",
      "+---+---+----+\n",
      "| 29| 18|23.5|\n",
      "| 29| 21|25.0|\n",
      "| 31| 20|25.5|\n",
      "| 30| 21|25.5|\n",
      "| 26| 16|21.0|\n",
      "+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "    FROM mpg\n",
    "    \"\"\"\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5532cbc-bf27-4a23-978f-553f5b2adfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e43105c-6e5b-4c3f-85a6-55407c71aa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc9c80b9-b219-4e1c-baee-a1304a8921cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy.cast('string')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ac6bd21-cf2e-4d6e-b338-409a485125aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|model|model|\n",
      "+-----+-----+\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note that if an entry can't be cast to a type it will be replace with null\n",
    "mpg.select(mpg.model, mpg.model.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716105f-ab67-4751-a8a4-1b34bc1226d5",
   "metadata": {},
   "source": [
    "Note that importing the sum function directly will override the built-in sum function. This means you will get an error if you try to sum a list of numbers, because sum will refernce the pyspark sum function, which works with pyspark dataframe columns, while the built-in sum function works with lists of numbers. The same holds true for the built in min and max functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f286821-263a-4be6-adc8-68272532a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The pyspark avg and mean functions are aliases of each other\n",
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "409519a5-2d35-4660-9352-b49d35433444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used to import all pyspark functions directly\n",
    "# the overwrite of the sum and min and max still applies unless\n",
    "# we just use: from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc0bf2a3-f56d-4d3f-87ca-34a8cd095654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+--------+--------+\n",
      "|        average_1|        average_2|min(hwy)|max(hwy)|\n",
      "+-----------------+-----------------+--------+--------+\n",
      "|23.44017094017094|23.44017094017094|      12|      44|\n",
      "+-----------------+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    (sum(mpg.hwy) / count(mpg.hwy)).alias('average_1'),\n",
    "    avg(mpg.hwy).alias('average_2'),\n",
    "    min(mpg.hwy),\n",
    "    max(mpg.hwy),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e311017-a7ab-4025-aa40-7a5644f49fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|concat(manufacturer, model)|\n",
      "+---------------------------+\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(concat(mpg.manufacturer, mpg.model)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68038cc-e45e-4521-8503-15ac2771f0f6",
   "metadata": {},
   "source": [
    "In order to use a string literal as part of our select, we'll need to use the lit function, otherwise spark will try to resolve our string as a column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc46580c-81e2-4040-9dc7-3cd74cd9c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe3ae7a0-0014-4bdc-b7df-37c5b7f13f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|concat(cyl,  cylinders)|\n",
      "+-----------------------+\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            6 cylinders|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(concat(mpg.cyl, lit(' cylinders'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b335b97-5414-4ef8-b3c1-cd9b0cb76cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02e0616a-5379-4552-9c1f-c98ac8f87a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|address                                      |\n",
      "+---------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a083138-4780-4fe3-89a5-1ac36ff14160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+------------------+\n",
      "|address                                      |street_no|street            |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|600      |Navarro St ste 600|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |3130     |Broadway St       |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |303      |Pearl Pkwy        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |1255     |SW Loop 410       |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    'address',\n",
    "    regexp_extract(\"address\", r\"^(\\d+)\", 1).alias('street_no'),\n",
    "    regexp_extract(\"address\", r\"^\\d+\\s([\\w\\s]+?),\", 1).alias(\"street\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c290d926-9942-47e5-bbc3-9ffdd8588fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------+\n",
      "|address                                      |city_state_zip       |\n",
      "+---------------------------------------------+---------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |San Antonio, TX 78209|\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |San Antonio, TX 78215|\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |San Antonio, TX 78227|\n",
      "+---------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_replace(\"address\", r\"^.*?,\\s*\", \"\").alias(\"city_state_zip\"),\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93067d5a-a640-46d0-b011-20820d9ec249",
   "metadata": {},
   "source": [
    "Spark provides two dataframe methods, .filter and .where, which both allow us to select a subset of the rows of our dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4a8395f-53c4-433d-9c07-67dba9267590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|      model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 28| 33|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 25| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 23| 29|  p|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|manual(m5)|  f| 26| 34|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 25| 36|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "|       honda|      civic|  2.0|2008|  4|manual(m6)|  f| 21| 29|  p|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|  auto(l4)|  f| 19| 26|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|manual(m5)|  f| 19| 29|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|manual(m5)|  f| 20| 28|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|  auto(l4)|  f| 20| 27|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|  auto(l4)|  4| 21| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|  auto(l4)|  4| 19| 26|  r|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|  volkswagen| new beetle|  2.0|1999|  4|manual(m5)|  f| 21| 29|  r|subcompact|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 4).where(mpg['class'] == 'subcompact').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfecd9-6e7b-4f21-94b2-0ac54407c418",
   "metadata": {},
   "source": [
    "Similar to an IF in Excel, CASE...WHEN in SQL, or np.where in python, spark provides a when function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d3726e9-58f5-4311-b69b-e8ddeae78b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c9ff6f0-9dc6-4df4-82ad-71817808b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|hwy|    mpg_desc|\n",
      "+---+------------+\n",
      "| 29|good_mileage|\n",
      "| 29|good_mileage|\n",
      "| 31|good_mileage|\n",
      "| 30|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 25|        null|\n",
      "| 28|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 25|        null|\n",
      "+---+------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, when(mpg.hwy > 25, 'good_mileage').alias('mpg_desc')).show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3c1ca-f72c-4832-a296-d7425369661f",
   "metadata": {},
   "source": [
    "To specify multiple conditions, we can chain .when calls. The first condition that is met will be the value that is used, and if none of the conditions are met the value specified in the .otherwise will be used (or null if you don't provide a .otherwise).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34bb9929-a6d4-4560-9223-85cdcbaf2f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|displ|engine_size|\n",
      "+-----+-----------+\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "|  2.0|     medium|\n",
      "|  2.8|     medium|\n",
      "|  2.8|     medium|\n",
      "|  3.1|      large|\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.displ,\n",
    "    (\n",
    "        when(mpg.displ < 2, 'small')\n",
    "        .when(mpg.displ < 3, 'medium')\n",
    "        .otherwise('large')\n",
    "        .alias('engine_size')\n",
    "    )\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea98c51a-bcc4-4803-9f39-fb5e96416af0",
   "metadata": {},
   "source": [
    "Spark lets us sort the rows in our dataframe by one or multiple columns with two methods: .sort, and .orderBy. .sort and .orderBy are aliases of each other and do the exact same thing. Like other methods we've seen, .sort takes in a Column object or a string that is the name of a column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d203ccb8-e763-4226-8bfb-40814e478cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|manufacturer|              model|displ|year|cyl|     trans|drv|cty|hwy| fl| class|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|       dodge|        durango 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|        jeep| grand cherokee 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|  dakota pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|manual(m6)|  4|  9| 12|  e|pickup|\n",
      "|        jeep| grand cherokee 4wd|  6.1|2008|  8|  auto(l5)|  4| 11| 14|  p|   suv|\n",
      "|   chevrolet|    k1500 tahoe 4wd|  5.3|2008|  8|  auto(l4)|  4| 11| 14|  e|   suv|\n",
      "|  land rover|        range rover|  4.6|1999|  8|  auto(l4)|  4| 11| 15|  p|   suv|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(mpg.hwy).show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1829d81-17a0-4ca7-9fe1-7657c5d7be1f",
   "metadata": {},
   "source": [
    "By default, values are sorted in ascending order. To sort in descending order, we can use the .desc method on any Column object, or the desc function from pyspark.sql.functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95cc0198-d11f-4657-89aa-ac5c1184e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "484a3163-4490-4d38-af27-81f09f349633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|  volkswagen|     jetta|  1.9|1999|  4|manual(m5)|  f| 33| 44|  d|   compact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|      toyota|   corolla|  1.8|2008|  4|manual(m5)|  f| 28| 37|  r|   compact|\n",
      "|       honda|     civic|  1.8|2008|  4|  auto(l5)|  f| 25| 36|  r|subcompact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(mpg.hwy.desc())\n",
    "# is the same as\n",
    "mpg.sort(col('hwy').desc())\n",
    "# is the same as\n",
    "mpg.sort(desc('hwy')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b108db-aa76-4db2-adf4-8af5d2a53577",
   "metadata": {},
   "source": [
    "To specify sorting by multiple columns, we provide each column as a separate argument to .sort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f190bcc2-8d8c-4840-9cfb-249bbac519dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|manufacturer|             model|displ|year|cyl|     trans|drv|cty|hwy| fl|class|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 20| 27|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 20| 26|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|manual(m5)|  4| 18| 25|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 19| 25|  p|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|  auto(l4)|  4| 18| 24|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 18| 23|  p|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|  auto(l4)|  4| 16| 20|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|manual(m5)|  4| 15| 20|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.0|2008|  6|  auto(l5)|  4| 17| 22|  d|  suv|\n",
      "|      toyota|       4runner 4wd|  4.0|2008|  6|  auto(l5)|  4| 16| 20|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  4.0|1999|  6|  auto(l4)|  4| 15| 20|  r|  suv|\n",
      "|      nissan|    pathfinder 4wd|  4.0|2008|  6|  auto(l5)|  4| 14| 20|  p|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.7|2008|  6|  auto(l5)|  4| 15| 19|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|manual(m5)|  4| 15| 19|  r|  suv|\n",
      "|     mercury|   mountaineer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  3.4|1999|  6|  auto(l4)|  4| 15| 19|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|     mercury|   mountaineer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "|       dodge|       durango 4wd|  3.9|1999|  6|  auto(l4)|  4| 13| 17|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(desc('class'), mpg.cyl.asc(), col('hwy').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a0c47-d82f-4254-9655-cf8d8f68b3cd",
   "metadata": {},
   "source": [
    "To aggregate our data by group, we can use the .groupBy method. Like with .select, we can pass either Column objects or strings that are column names to .groupBy. All of the expressions below are equivalent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e677278-ab32-4435-960a-efe7a7ce8d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x11868afa0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy('cyl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4155ed9-95d0-4030-a0e5-77e07da915a9",
   "metadata": {},
   "source": [
    "Once the data is grouped, we need to specify an aggregation. We can use one of the aggregate functions we imported earlier, along with a column:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ac69e2a-1161-4030-8020-b05ce752cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+\n",
      "|cyl|          avg(cty)|         avg(hwy)|\n",
      "+---+------------------+-----------------+\n",
      "|  6| 16.21518987341772|22.82278481012658|\n",
      "|  8|12.571428571428571|17.62857142857143|\n",
      "|  4|21.012345679012345|28.80246913580247|\n",
      "|  5|              20.5|            28.75|\n",
      "+---+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl).agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870f2c7-8e76-470a-b8fb-cc6fb386adc7",
   "metadata": {},
   "source": [
    "To group by multiple columns, pass each of the columns a a separate argument to .groupBy (Note that this is different from pandas, where we would need to pass a list).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54ea5d67-f1e6-40e8-8748-b33da2399270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+\n",
      "|cyl|     class|          avg(cty)|          avg(hwy)|\n",
      "+---+----------+------------------+------------------+\n",
      "|  8|       suv|12.131578947368421|16.789473684210527|\n",
      "|  8|   midsize|              16.0|              24.0|\n",
      "|  8|   2seater|              15.4|              24.8|\n",
      "|  6|   compact|16.923076923076923|25.307692307692307|\n",
      "|  4|   compact|            21.375|          29.46875|\n",
      "|  6|   midsize|17.782608695652176| 26.26086956521739|\n",
      "|  6|    pickup|              14.5|              17.9|\n",
      "|  8|    pickup|              11.8|              15.8|\n",
      "|  4|   midsize|              20.5|           29.1875|\n",
      "|  6|   minivan|              15.6|              22.2|\n",
      "|  4|   minivan|              18.0|              24.0|\n",
      "|  6|       suv|              14.5|              18.5|\n",
      "|  6|subcompact|              17.0|24.714285714285715|\n",
      "|  4|subcompact|22.857142857142858| 30.80952380952381|\n",
      "|  8|subcompact|              14.8|              21.6|\n",
      "|  4|       suv|              18.0|             23.75|\n",
      "|  4|    pickup|              16.0|20.666666666666668|\n",
      "|  5|   compact|              21.0|              29.0|\n",
      "|  5|subcompact|              20.0|              28.5|\n",
      "+---+----------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.groupBy('cyl', 'class').agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79aa5ef-ace5-42a6-83e7-064d6930aeab",
   "metadata": {},
   "source": [
    "In addition to .groupBy, we can use .rollup, which will do the same aggregations, but will also include the overall total:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dbbb78a-4521-406f-8497-bfcdfbcf24c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| cyl|count|\n",
      "+----+-----+\n",
      "|null|  234|\n",
      "|   4|   81|\n",
      "|   5|    4|\n",
      "|   6|   79|\n",
      "|   8|   70|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.rollup('cyl').count().sort('cyl').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a15f04-50c4-4455-8be3-eb61729c539a",
   "metadata": {},
   "source": [
    "Here the null value in cyl indicates the total count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2572e870-afb4-403b-98dc-db31ce35d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "| cyl|         avg(hwy)|\n",
      "+----+-----------------+\n",
      "|null|23.44017094017094|\n",
      "|   4|28.80246913580247|\n",
      "|   5|            28.75|\n",
      "|   6|22.82278481012658|\n",
      "|   8|17.62857142857143|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.rollup('cyl').agg(expr('avg(hwy)')).sort('cyl').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef6ce2-7aac-4409-9601-eb49138a1014",
   "metadata": {},
   "source": [
    "And in the example above, the null row represents the overall average highway mileage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9663c9a8-750a-45c7-9bb3-150491aac089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "| cyl|     class|          avg(hwy)|\n",
      "+----+----------+------------------+\n",
      "|null|      null| 23.44017094017094|\n",
      "|   4|      null| 28.80246913580247|\n",
      "|   4|   compact|          29.46875|\n",
      "|   4|   midsize|           29.1875|\n",
      "|   4|   minivan|              24.0|\n",
      "|   4|    pickup|20.666666666666668|\n",
      "|   4|subcompact| 30.80952380952381|\n",
      "|   4|       suv|             23.75|\n",
      "|   5|      null|             28.75|\n",
      "|   5|   compact|              29.0|\n",
      "|   5|subcompact|              28.5|\n",
      "|   6|      null| 22.82278481012658|\n",
      "|   6|   compact|25.307692307692307|\n",
      "|   6|   midsize| 26.26086956521739|\n",
      "|   6|   minivan|              22.2|\n",
      "|   6|    pickup|              17.9|\n",
      "|   6|subcompact|24.714285714285715|\n",
      "|   6|       suv|              18.5|\n",
      "|   8|      null| 17.62857142857143|\n",
      "|   8|   2seater|              24.8|\n",
      "+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.rollup('cyl', 'class').mean('hwy').sort(col('cyl'), col('class')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb759a2-dc2a-4c78-8d98-6c5c7a5e246e",
   "metadata": {},
   "source": [
    "In addition to groupby, spark provides a couple other ways to do aggregation. One of which is .crosstab. This is very similary to pandas .crosstab function, in that it calculates the number of occurances of each unique value from the two passed columns:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d795019-a6f0-4d21-b238-8a19bdf5ded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+---+\n",
      "| class_cyl|  4|  5|  6|  8|\n",
      "+----------+---+---+---+---+\n",
      "|subcompact| 21|  2|  7|  5|\n",
      "|   compact| 32|  2| 13|  0|\n",
      "|   minivan|  1|  0| 10|  0|\n",
      "|       suv|  8|  0| 16| 38|\n",
      "|   midsize| 16|  0| 23|  2|\n",
      "|    pickup|  3|  0| 10| 20|\n",
      "|   2seater|  0|  0|  0|  5|\n",
      "+----------+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.crosstab('class', 'cyl').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58145d-18c0-47b0-87c3-9d3ccbcd19f9",
   "metadata": {},
   "source": [
    ".crosstab simply does counts, if we want a different aggregation, we can use .pivot. For example, to find the average highway mileage for each combination of car class and number of cylinders, we could write the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddd979ee-9252-4669-8354-fe5f04f329ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+------------------+------------------+\n",
      "|     class|                 4|   5|                 6|                 8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "|subcompact| 30.80952380952381|28.5|24.714285714285715|              21.6|\n",
      "|   compact|          29.46875|29.0|25.307692307692307|              null|\n",
      "|   minivan|              24.0|null|              22.2|              null|\n",
      "|       suv|             23.75|null|              18.5|16.789473684210527|\n",
      "|   midsize|           29.1875|null| 26.26086956521739|              24.0|\n",
      "|    pickup|20.666666666666668|null|              17.9|              15.8|\n",
      "|   2seater|              null|null|              null|              24.8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupby('class').pivot('cyl').mean('hwy').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d53ba-82ac-4347-8e44-1558c6a8f8cf",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ac4680d-1299-4504-82c5-ae7a526264b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|NaN|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e45129c3-0392-46b6-a9a0-0259189f8ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bf9c2e3-fc88-4d18-9187-dbffada9f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|0.0|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|0.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2600e6-00e0-4f8a-a1cd-04e9133b024d",
   "metadata": {},
   "source": [
    "For both methods, we can specify that we only want to fill or drop values in a specific column with a second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e6adc8b-f199-44d3-83ba-6820181e3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0, subset=\"x\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96ad6081-1fae-4b01-9f1f-774eaebb10c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=\"y\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137839f-4f87-4893-a2f5-520d46677ed5",
   "metadata": {},
   "source": [
    "#### Explaining DataFrame Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75592bc7-f286-4705-a4b7-fda5de2b4f09",
   "metadata": {},
   "source": [
    "The .explain method will show us how spark is thinking about our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8109196-d452-4128-835d-64bca2864b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[manufacturer#152,model#153,displ#154,year#155L,cyl#156L,trans#157,drv#158,cty#159L,hwy#160L,fl#161,class#162]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e611610-ca31-4ab0-a42c-35efa3348938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#156L, hwy#160L]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#152,model#153,displ#154,year#155L,cyl#156L,trans#157,drv#158,cty#159L,hwy#160L,fl#161,class#162]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.cyl, mpg.hwy).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5923b6-a0bd-4cfb-b9ff-20aa1cb3b41f",
   "metadata": {},
   "source": [
    "Here we are doing a more advanced select calculation, but this is still just a single step to spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1cb0b78-0c02-4048-9b6c-af607688ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(cyl#156L) AND (cyl#156L = 6))\n",
      "+- *(1) Scan ExistingRDD[manufacturer#152,model#153,displ#154,year#155L,cyl#156L,trans#157,drv#158,cty#159L,hwy#160L,fl#161,class#162]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 6).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb31801-876a-4116-a550-16e43adfa328",
   "metadata": {},
   "source": [
    "#### More Dataframe Manipulation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79f9e890-4967-4f2c-ad6f-c2a3e3bb267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0983df94-fd10-40d2-a8f5-a65cbf7e374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date, max_date = weather.select(min(\"date\"), max(\"date\")).first()\n",
    "# min_date, max_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aee63d63-7f6e-4478-9429-bce96eeb48ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|temp_avg|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|     9.0|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|     6.5|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|     9.5|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|     9.0|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|     6.0|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|     3.5|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather = weather.withColumn(\n",
    "    \"temp_avg\", expr(\"ROUND(temp_min + temp_max) / 2\")\n",
    ")\n",
    "weather.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4fdd6335-9a25-4554-91f0-98d6632271d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "397707ab-af78-4f2f-a767-7304305643d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|    total_rainfall|\n",
      "+-----+------------------+\n",
      "|    1|465.99999999999994|\n",
      "|    2|             422.0|\n",
      "|    3|             606.2|\n",
      "|    4|             375.4|\n",
      "|    5|             207.5|\n",
      "|    6|             132.9|\n",
      "|    7|              48.2|\n",
      "|    8|             163.7|\n",
      "|    9|235.49999999999997|\n",
      "|   10|             503.4|\n",
      "|   11|             642.5|\n",
      "|   12| 622.7000000000002|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"precipitation\").alias(\"total_rainfall\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3eabad-4de5-41d9-99f8-687b976ec17e",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc968a89-5f70-41b8-af54-66c0d2eed48f",
   "metadata": {},
   "source": [
    "##### 1. Create a spark data frame that contains your favorite programming languages.\n",
    "- The name of the column should be language\n",
    "- View the schema of the dataframe\n",
    "- Output the shape of the dataframe\n",
    "- Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed3ff0c4-858f-4985-a1eb-33c3c67cc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "langs = {'languages':['python', 'julia', 'mysql', 'html', 'pyspark']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9ce0d48-8da1-452c-b427-f2a899717a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = pd.DataFrame(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4366a65b-110b-4944-a89e-06c753c6a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = spark.createDataFrame(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5028b286-bd60-46a4-8bba-22f745ca3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = langs.withColumnRenamed('languages', 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fedc74ce-b2e7-40bf-969a-0ce815b58194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "langs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "945432ad-4282-472c-81fe-bf785e6b241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 rows, 1 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'{langs.count()} rows, {len(langs.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6187bfd8-47b6-41a9-90e3-52a31e5d387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|language|\n",
      "+--------+\n",
      "|  python|\n",
      "|   julia|\n",
      "|   mysql|\n",
      "|    html|\n",
      "| pyspark|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "langs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37926adb-de97-4e36-8659-4c22801a0b6d",
   "metadata": {},
   "source": [
    "##### 2. Load the mpg dataset as a spark dataframe.\n",
    "    a. Create 1 column of output that contains a message like the one below:\n",
    "    \n",
    "> The 1999 audi a4 has a 4 cylinder engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1be4dcb6-dacd-47be-a05e-98f6b1d27cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5196c5fc-f02f-4887-9140-ed1c529d67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = spark.createDataFrame(data('mpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d70187f-2e94-4468-8da6-1bc5bfba03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a53eea6e-1087-4f1b-a37e-efa0d4d46e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|statement                               |\n",
      "+----------------------------------------+\n",
      "|The 1999 audi a4 has a 4 cylinder engine|\n",
      "|The 1999 audi a4 has a 4 cylinder engine|\n",
      "|The 2008 audi a4 has a 4 cylinder engine|\n",
      "|The 2008 audi a4 has a 4 cylinder engine|\n",
      "|The 1999 audi a4 has a 6 cylinder engine|\n",
      "+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    (concat(\n",
    "        lit('The '),\n",
    "        mpg.year,\n",
    "        lit(' '),\n",
    "        mpg.manufacturer,\n",
    "        lit(' '),\n",
    "        mpg.model,\n",
    "        lit(' has a '),\n",
    "        mpg.cyl,\n",
    "        lit(' cylinder engine')\n",
    "    )).alias('statement')).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb05ae-7b41-4e75-baf8-38132664a230",
   "metadata": {},
   "source": [
    "    b. Transform the trans column so that it only contains either manual or auto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a740bd0f-e114-43c0-b62f-a2c449f129d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|trans_type|\n",
      "+----------+\n",
      "|      auto|\n",
      "|    manual|\n",
      "|    manual|\n",
      "|      auto|\n",
      "|      auto|\n",
      "|    manual|\n",
      "|      auto|\n",
      "|    manual|\n",
      "|      auto|\n",
      "|    manual|\n",
      "|      auto|\n",
      "|      auto|\n",
      "|    manual|\n",
      "|      auto|\n",
      "|    manual|\n",
      "|      auto|\n",
      "|      auto|\n",
      "|      auto|\n",
      "|      auto|\n",
      "|      auto|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(when(mpg.trans.contains('auto'), 'auto')\n",
    "          .otherwise('manual')\n",
    "          .alias('trans_type')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c4ff6-ade4-4d52-8c44-93165ee570bc",
   "metadata": {},
   "source": [
    "##### 3. Load the tips dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46493fc7-744b-40d0-888d-13f6daadf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = spark.createDataFrame(data('tips'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89e809da-deb7-424a-8d0d-d9954ad29487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee76448-308a-4a41-8bea-bda3b9428a71",
   "metadata": {},
   "source": [
    "    a. What percentage of observations are smokers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93103bcb-edf1-481b-af42-5430cc264daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.114754098360656"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.filter(tips.smoker == 'Yes').count() / tips.count() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f587a-2a72-4979-9206-e5f461886b64",
   "metadata": {},
   "source": [
    "    b. Create a column that contains the tip percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b96cfebd-3ea1-4448-9d64-3244996aac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------+\n",
      "|total_bill| tip|percent_tip|\n",
      "+----------+----+-----------+\n",
      "|     16.99|1.01|       5.94|\n",
      "|     10.34|1.66|      16.05|\n",
      "|     21.01| 3.5|      16.66|\n",
      "|     23.68|3.31|      13.98|\n",
      "|     24.59|3.61|      14.68|\n",
      "|     25.29|4.71|      18.62|\n",
      "|      8.77| 2.0|      22.81|\n",
      "|     26.88|3.12|      11.61|\n",
      "|     15.04|1.96|      13.03|\n",
      "|     14.78|3.23|      21.85|\n",
      "|     10.27|1.71|      16.65|\n",
      "|     35.26| 5.0|      14.18|\n",
      "|     15.42|1.57|      10.18|\n",
      "|     18.43| 3.0|      16.28|\n",
      "|     14.83|3.02|      20.36|\n",
      "|     21.58|3.92|      18.16|\n",
      "|     10.33|1.67|      16.17|\n",
      "|     16.29|3.71|      22.77|\n",
      "|     16.97| 3.5|      20.62|\n",
      "|     20.65|3.35|      16.22|\n",
      "+----------+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.withColumn('percent_tip', round((tips.tip / tips.total_bill * 100), 2)).\\\n",
    "    select('total_bill', 'tip', 'percent_tip').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9bf56-71eb-41b3-b52e-9664f765cc6b",
   "metadata": {},
   "source": [
    "    c. Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45900846-b791-4bdd-9baa-f30ca7c67f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 120:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------+\n",
      "|   sex|smoker|avg_perc_tip|\n",
      "+------+------+------------+\n",
      "|  Male|    No|       16.07|\n",
      "|Female|    No|       15.69|\n",
      "|  Male|   Yes|       15.28|\n",
      "|Female|   Yes|       18.21|\n",
      "+------+------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tips.withColumn('percent_tip', round((tips.tip / tips.total_bill * 100), 2)).\\\n",
    "    groupby('sex', 'smoker').agg(round(mean('percent_tip'), 2).alias('avg_perc_tip')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a833c1-8e83-42ab-b4cb-48e58d0e548d",
   "metadata": {},
   "source": [
    "##### 4. Use the seattle weather dataset referenced in the lesson to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1bd73179-41fc-4002-ba59-3e389a4c8059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|temp_avg|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|     9.0|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|     6.5|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|     9.5|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|     9.0|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|     6.0|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|     3.5|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85aab81-a1ff-4abb-9f1e-5b3dab18ff10",
   "metadata": {},
   "source": [
    "    - Convert the temperatures to fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc76c4e2-abb5-4ff1-a092-00231d793192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c to f: (xC  9/5) + 32 = 32F\n",
    "weather = weather.withColumn('temp_min', round(weather.temp_min * (9/5) + 32, 2)\n",
    "                            ).withColumn('temp_max', round(weather.temp_max * (9/5) + 32, 2)\n",
    "                  ).withColumn('temp_avg', round(weather.temp_avg * (9/5) + 32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "539975e5-e02a-492e-8662-5db63742bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------\n",
      " date          | 2012-01-01 \n",
      " precipitation | 0.0        \n",
      " temp_max      | 55.04      \n",
      " temp_min      | 41.0       \n",
      " wind          | 4.7        \n",
      " weather       | drizzle    \n",
      " temp_avg      | 48.2       \n",
      "-RECORD 1-------------------\n",
      " date          | 2012-01-02 \n",
      " precipitation | 10.9       \n",
      " temp_max      | 51.08      \n",
      " temp_min      | 37.04      \n",
      " wind          | 4.5        \n",
      " weather       | rain       \n",
      " temp_avg      | 43.7       \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daede9ae-0884-478f-a9ce-b3f05d1896c5",
   "metadata": {},
   "source": [
    "    - Which month has the most rain, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a393b084-0849-43d3-b524-d48a92f0d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5b02b1af-6332-4d85-95e6-71071ad99642",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.withColumn('month', F.month(weather.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7284ff85-beb8-449d-8999-3a9dc68d8300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|month|total_rainfall|\n",
      "+-----+--------------+\n",
      "|    1|         231.5|\n",
      "|   11|         210.5|\n",
      "|   10|         193.5|\n",
      "|    3|         182.1|\n",
      "|    2|         126.9|\n",
      "|   12|         115.6|\n",
      "|    6|          75.1|\n",
      "|    4|          68.6|\n",
      "|    5|          52.2|\n",
      "|    8|          38.6|\n",
      "|    7|          26.3|\n",
      "|    9|           0.9|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# going to define 'most rain' as total precipitation on days labeled rain\n",
    "weather.filter(weather.weather == 'rain').groupby('month').\\\n",
    "        agg(round(F.sum('precipitation'), 2).alias('total_rainfall')).\\\n",
    "        sort(F.col('total_rainfall').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe10e3d-9ba7-401d-8237-dce8aae47d19",
   "metadata": {},
   "source": [
    "Looks like January has the most rain by my definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0cc3b-5703-4a8f-b569-421ef68a4907",
   "metadata": {},
   "source": [
    "    - Which year was the windiest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d01c2c2-9dcb-4f71-9d9b-8ba541ac161d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|year|ttl_wind|\n",
      "+----+--------+\n",
      "|2014|  1232.8|\n",
      "|2012|  1226.0|\n",
      "|2015|  1139.2|\n",
      "|2013|   828.0|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.withColumn('year', F.year(weather.date)).\\\n",
    "        groupby('year').agg(round(F.sum('precipitation'), 2).alias('ttl_wind')).\\\n",
    "        sort(F.col('ttl_wind').desc()).show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc0261-9826-4265-b83f-e19186c61114",
   "metadata": {},
   "source": [
    "    - What is the most frequent type of weather in January?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b6fe4a1-2992-458f-9a6e-f0e0ed541af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|weather|count|\n",
      "+-------+-----+\n",
      "|    fog|   38|\n",
      "|   rain|   35|\n",
      "|    sun|   33|\n",
      "|drizzle|   10|\n",
      "|   snow|    8|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.filter(\n",
    "    F.month(weather.date) == 1\n",
    ").groupby('weather').count().sort(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685efa06-1fad-4ddc-919e-08dd05e64acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
